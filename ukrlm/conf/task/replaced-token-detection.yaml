name: "replaced-token-detection"
learning_rate: 1e-4
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.999
max_epochs: 1
max_steps: -1
gradient_accumulation_steps: 1
gradient_clip_val: 1.0
gradient_clip_algorithm: "norm"
accumulate_grad_batches: 1
collator: "DataCollatorForLanguageModeling"
mlm_probability: 0.15
rtd_lambda: 50.0
pad_to_multiple_of: 8
strategy: "ddp"
precision: bf16-mixed
use_flash_attention: true
log_every_n_steps: 200
save_every_n_steps: 50000
val_check_interval: 25000
sync_batchnorm: false
# fp16_opt_level: "O1"  # todo how do we use this?
