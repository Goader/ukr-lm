name: "masked-language-modeling"
objectives: ["masked-lm", "next-sentence-prediction"]  # TODO dummy values so far // where do we store objectives configuration?
learning_rate: 1e-4
weight_decay: 0.01
max_steps: 1000000
gradient_accumulation_steps: 1
gradient_clip_val: 1.0
gradient_clip_algorithm: "norm"
accumulate_grad_batches: 1
mlm_probability: 0.15
pad_to_multiple_of: null
strategy: "ddp"
precision: bf16-mixed
use_flash_attention: true
log_every_n_steps: 200
save_every_n_steps: 50000
val_check_interval: 25000
sync_batchnorm: false
# fp16_opt_level: "O1"  # todo how do we use this?
