name: "masked-language-modeling"
objectives: ["masked-lm", "next-sentence-prediction"]  # TODO dummy values so far // where do we store objectives configuration?
learning_rate: 1e-4
weight_decay: 0.01
max_steps: 1000000
gradient_accumulation_steps: 1
gradient_clip_val: 1.0
gradient_clip_algorithm: "norm"
mlm_probability: 0.15
pad_to_multiple_of: null
strategy: "ddp"
# max_grad_norm: 1.0  # todo do we want to use gradient clipping?
precision: bf16-mixed
# fp16_opt_level: "O1"  # todo how do we use this?
