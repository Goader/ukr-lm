{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Researching the construction and influence of different tokenizer pipeline nodes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "963ccf8d40823b0b"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    "    Regex,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T19:35:45.411749423Z",
     "start_time": "2023-08-03T19:35:45.082926159Z"
    }
   },
   "id": "63218f4429f5dee3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Other Models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a2ecfe6941e1ed3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### HerBERT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58563338ecbf5380"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('allegro/herbert-base-cased')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T19:36:27.931573147Z",
     "start_time": "2023-08-03T19:36:27.408897163Z"
    }
   },
   "id": "9046210858a23d69"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "tokenizer_obj = tokenizer._tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T19:36:46.333578770Z",
     "start_time": "2023-08-03T19:36:46.306656808Z"
    }
   },
   "id": "7ba03b3c85e964d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Normalizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d131011057f43b0a"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tokenizers.normalizers.BertNormalizer object at 0x7f38778749f0>\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', 'clean_text', 'custom', 'handle_chinese_chars', 'lowercase', 'normalize', 'normalize_str', 'strip_accents']\n",
      "norm.clean_text=True\n",
      "norm.handle_chinese_chars=True\n",
      "norm.strip_accents=False\n",
      "norm.lowercase=False\n"
     ]
    }
   ],
   "source": [
    "norm = tokenizer_obj.normalizer\n",
    "print(norm)\n",
    "print(dir(norm))\n",
    "print(f'{norm.clean_text=}')\n",
    "print(f'{norm.handle_chinese_chars=}')\n",
    "print(f'{norm.strip_accents=}')\n",
    "print(f'{norm.lowercase=}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T19:38:24.139490964Z",
     "start_time": "2023-08-03T19:38:24.098215824Z"
    }
   },
   "id": "dc284ad984705f8d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PreTokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc4bc46647bf7114"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tokenizers.pre_tokenizers.BertPreTokenizer object at 0x7f3878559db0>\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', 'custom', 'pre_tokenize', 'pre_tokenize_str']\n"
     ]
    }
   ],
   "source": [
    "pre_tokenizer = tokenizer_obj.pre_tokenizer\n",
    "print(pre_tokenizer)\n",
    "print(dir(pre_tokenizer))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T19:51:54.125787037Z",
     "start_time": "2023-08-03T19:51:54.105639056Z"
    }
   },
   "id": "c2248c4c348ed6f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cbefd79f0357efb"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tokenizers.models.BPE object at 0x7f38789b3cd0>\n",
      "['__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', 'continuing_subword_prefix', 'dropout', 'end_of_word_suffix', 'from_file', 'fuse_unk', 'get_trainer', 'id_to_token', 'read_file', 'save', 'token_to_id', 'tokenize', 'unk_token']\n",
      "model.continuing_subword_prefix=None\n",
      "model.dropout=None\n",
      "model.end_of_word_suffix='</w>'\n",
      "model.fuse_unk=False\n",
      "model.unk_token='<unk>'\n"
     ]
    }
   ],
   "source": [
    "model = tokenizer_obj.model\n",
    "print(model)\n",
    "print(dir(model))\n",
    "print(f'{model.continuing_subword_prefix=}')\n",
    "print(f'{model.dropout=}')\n",
    "print(f'{model.end_of_word_suffix=}')\n",
    "print(f'{model.fuse_unk=}')\n",
    "print(f'{model.unk_token=}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T19:55:57.368942835Z",
     "start_time": "2023-08-03T19:55:57.287068702Z"
    }
   },
   "id": "51e6abcc95dec839"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PostProcessor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b8bc5d8ed795f22"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tokenizers.processors.BertProcessing object at 0x7f3878def810>\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', 'num_special_tokens_to_add', 'process']\n"
     ]
    }
   ],
   "source": [
    "post_processor = tokenizer_obj.post_processor\n",
    "print(post_processor)\n",
    "print(dir(post_processor))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T19:53:27.694895861Z",
     "start_time": "2023-08-03T19:53:27.678615614Z"
    }
   },
   "id": "337f5ec1e77d55ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RoBERTa"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8b81207c8c324f7"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "216aa59eb7570c83"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "tokenizer_obj = tokenizer._tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3644ae9b31e8abbd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7dd21a23bf1b0236"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a8f5228d09678bf"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "print(tokenizer_obj.normalizer)\n",
    "print(dir(tokenizer_obj.normalizer))\n",
    "print(f'{tokenizer_obj.normalizer.clean_text=}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b5c6e9480c4a842"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ukrainian and different pipeline nodes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4307d99ba7f3514e"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "normalizer_test_strings = [\n",
    "    'Це тестовий текст',\n",
    "    'Я пишуся з Великої Букви',\n",
    "    'А а, Б б, В в, Г г, Ґ ґ, Д д, Е е, Є є, Ж ж, З з, И и, І і, Ї ї, Й й, К к, Л л, М м, Н н, О о, П п, Р р, С с, Т т, У у, Ф ф, Х х, Ц ц, Ч ч, Ш ш, Щ щ, ь, Ю ю, Я я',\n",
    "    '0, 1, 2, 3, 4, 5, 6, 7, 8, 9',\n",
    "    'Але є ще такі символи: ! \" # $ % & \\' ( ) * + , - . / : ; < = > ? @ [ \\\\ ] ^ _ ` { | } ~',\n",
    "    'Грім-2',\n",
    "    'ы ъ ё',\n",
    "    'прислів\\'я'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T20:15:43.255105576Z",
     "start_time": "2023-08-03T20:15:43.214272575Z"
    }
   },
   "id": "6cfaf01de35cb43f"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "pre_tokenizer_test_strings = [\n",
    "    'Це тестовий текст',\n",
    "    'Я пишуся з Великої Букви',\n",
    "    'Грім-2',\n",
    "    'Грім2',\n",
    "    'прислів\\'я',\n",
    "    'синьо-жовтий'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T20:22:05.153154428Z",
     "start_time": "2023-08-03T20:22:05.000995396Z"
    }
   },
   "id": "7d693df0b248103e"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def print_normalizer(normalizer, test_strings):\n",
    "    for test_string in test_strings:\n",
    "        print(f'Original:    {test_string}')\n",
    "        print(f'Normalized:  {normalizer.normalize_str(test_string)}')\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T20:15:44.412051143Z",
     "start_time": "2023-08-03T20:15:44.410181293Z"
    }
   },
   "id": "97c1f714c011aad7"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def print_pre_tokenizer(pre_tokenizer, test_strings):\n",
    "    for test_string in test_strings:\n",
    "        print(f'Original:       {test_string}')\n",
    "        print(f'Pre-tokenized:  {pre_tokenizer.pre_tokenize_str(test_string)}')\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T20:15:44.867134380Z",
     "start_time": "2023-08-03T20:15:44.860524747Z"
    }
   },
   "id": "38a0dbf7311eb63c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BertNormalizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e63f749f244ae7d"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:    Це тестовий текст\n",
      "Normalized:  Це тестовии текст\n",
      "\n",
      "Original:    Я пишуся з Великої Букви\n",
      "Normalized:  Я пишуся з Великоі Букви\n",
      "\n",
      "Original:    А а, Б б, В в, Г г, Ґ ґ, Д д, Е е, Є є, Ж ж, З з, И и, І і, Ї ї, Й й, К к, Л л, М м, Н н, О о, П п, Р р, С с, Т т, У у, Ф ф, Х х, Ц ц, Ч ч, Ш ш, Щ щ, ь, Ю ю, Я я\n",
      "Normalized:  А а, Б б, В в, Г г, Ґ ґ, Д д, Е е, Є є, Ж ж, З з, И и, І і, І і, И и, К к, Л л, М м, Н н, О о, П п, Р р, С с, Т т, У у, Ф ф, Х х, Ц ц, Ч ч, Ш ш, Щ щ, ь, Ю ю, Я я\n",
      "\n",
      "Original:    0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
      "Normalized:  0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
      "\n",
      "Original:    Але є ще такі символи: ! \" # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \\ ] ^ _ ` { | } ~\n",
      "Normalized:  Але є ще такі символи: ! \" # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \\ ] ^ _ ` { | } ~\n",
      "\n",
      "Original:    Грім-2\n",
      "Normalized:  Грім-2\n",
      "\n",
      "Original:    ы ъ ё\n",
      "Normalized:  ы ъ е\n",
      "\n",
      "Original:    прислів'я\n",
      "Normalized:  прислів'я\n"
     ]
    }
   ],
   "source": [
    "norm = normalizers.BertNormalizer(lowercase=False, strip_accents=True)\n",
    "print_normalizer(norm, normalizer_test_strings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T20:15:45.435042939Z",
     "start_time": "2023-08-03T20:15:45.428966140Z"
    }
   },
   "id": "8412fbd836459698"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We cannot allow ourselves to strip accents, because we need to preserve the difference between 'ї' and 'і', as well as 'й' and 'и'."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b15e525b04371f"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:    Це тестовий текст\n",
      "Normalized:  Це тестовий текст\n",
      "\n",
      "Original:    Я пишуся з Великої Букви\n",
      "Normalized:  Я пишуся з Великої Букви\n",
      "\n",
      "Original:    А а, Б б, В в, Г г, Ґ ґ, Д д, Е е, Є є, Ж ж, З з, И и, І і, Ї ї, Й й, К к, Л л, М м, Н н, О о, П п, Р р, С с, Т т, У у, Ф ф, Х х, Ц ц, Ч ч, Ш ш, Щ щ, ь, Ю ю, Я я\n",
      "Normalized:  А а, Б б, В в, Г г, Ґ ґ, Д д, Е е, Є є, Ж ж, З з, И и, І і, Ї ї, Й й, К к, Л л, М м, Н н, О о, П п, Р р, С с, Т т, У у, Ф ф, Х х, Ц ц, Ч ч, Ш ш, Щ щ, ь, Ю ю, Я я\n",
      "\n",
      "Original:    0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
      "Normalized:  0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
      "\n",
      "Original:    Але є ще такі символи: ! \" # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \\ ] ^ _ ` { | } ~\n",
      "Normalized:  Але є ще такі символи: ! \" # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \\ ] ^ _ ` { | } ~\n",
      "\n",
      "Original:    Грім-2\n",
      "Normalized:  Грім-2\n",
      "\n",
      "Original:    ы ъ ё\n",
      "Normalized:  ы ъ ё\n",
      "\n",
      "Original:    прислів'я\n",
      "Normalized:  прислів'я\n"
     ]
    }
   ],
   "source": [
    "norm = normalizers.BertNormalizer(lowercase=False, strip_accents=False)\n",
    "print_normalizer(norm, normalizer_test_strings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T20:15:46.796865159Z",
     "start_time": "2023-08-03T20:15:46.788883910Z"
    }
   },
   "id": "1b933b411794a5b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BertPreTokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2009f6edc6b55386"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:       Це тестовий текст\n",
      "Pre-tokenized:  [('Це', (0, 2)), ('тестовий', (3, 11)), ('текст', (12, 17))]\n",
      "\n",
      "Original:       Я пишуся з Великої Букви\n",
      "Pre-tokenized:  [('Я', (0, 1)), ('пишуся', (2, 8)), ('з', (9, 10)), ('Великої', (11, 18)), ('Букви', (19, 24))]\n",
      "\n",
      "Original:       Грім-2\n",
      "Pre-tokenized:  [('Грім', (0, 4)), ('-', (4, 5)), ('2', (5, 6))]\n",
      "\n",
      "Original:       Грім2\n",
      "Pre-tokenized:  [('Грім2', (0, 5))]\n",
      "\n",
      "Original:       прислів'я\n",
      "Pre-tokenized:  [('прислів', (0, 7)), (\"'\", (7, 8)), ('я', (8, 9))]\n",
      "\n",
      "Original:       синьо-жовтий\n",
      "Pre-tokenized:  [('синьо', (0, 5)), ('-', (5, 6)), ('жовтий', (6, 12))]\n"
     ]
    }
   ],
   "source": [
    "pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "print_pre_tokenizer(pre_tokenizer, pre_tokenizer_test_strings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T20:22:10.325840646Z",
     "start_time": "2023-08-03T20:22:10.320046985Z"
    }
   },
   "id": "78ebee4c0aa4eb51"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ByteLevel"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43297ce0b8e9478f"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:       Це тестовий текст\n",
      "Pre-tokenized:  [('Ð¦Ðµ', (0, 2)), ('ĠÑĤÐµÑģÑĤÐ¾Ð²Ð¸Ð¹', (2, 11)), ('ĠÑĤÐµÐºÑģÑĤ', (11, 17))]\n",
      "\n",
      "Original:       Я пишуся з Великої Букви\n",
      "Pre-tokenized:  [('Ð¯', (0, 1)), ('ĠÐ¿Ð¸ÑĪÑĥÑģÑı', (1, 8)), ('ĠÐ·', (8, 10)), ('ĠÐĴÐµÐ»Ð¸ÐºÐ¾ÑĹ', (10, 18)), ('ĠÐĳÑĥÐºÐ²Ð¸', (18, 24))]\n",
      "\n",
      "Original:       Грім-2\n",
      "Pre-tokenized:  [('ÐĵÑĢÑĸÐ¼', (0, 4)), ('-', (4, 5)), ('2', (5, 6))]\n",
      "\n",
      "Original:       Грім2\n",
      "Pre-tokenized:  [('ÐĵÑĢÑĸÐ¼', (0, 4)), ('2', (4, 5))]\n",
      "\n",
      "Original:       прислів'я\n",
      "Pre-tokenized:  [('Ð¿ÑĢÐ¸ÑģÐ»ÑĸÐ²', (0, 7)), (\"'\", (7, 8)), ('Ñı', (8, 9))]\n",
      "\n",
      "Original:       синьо-жовтий\n",
      "Pre-tokenized:  [('ÑģÐ¸Ð½ÑĮÐ¾', (0, 5)), ('-', (5, 6)), ('Ð¶Ð¾Ð²ÑĤÐ¸Ð¹', (6, 12))]\n"
     ]
    }
   ],
   "source": [
    "pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "print_pre_tokenizer(pre_tokenizer, pre_tokenizer_test_strings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T20:22:10.908280715Z",
     "start_time": "2023-08-03T20:22:10.901135446Z"
    }
   },
   "id": "40603d42a55a3a50"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T20:21:53.338102253Z",
     "start_time": "2023-08-03T20:21:53.316817026Z"
    }
   },
   "id": "bbc6906c2653f116"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "324ae373c87fab90"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
